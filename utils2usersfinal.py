from tensorflow.python.keras import *
import tensorflow as tf
import scipy.io as sio
import numpy as np
from numpy import asarray
import sys
# ---------------------
#  Global Parameters
# ---------------------
Nt = 64  # the number of antennas
P = 1   # the normalized transmit power
																								

# ---------------------
#  Functions
# ---------------------

# transfer the phase to complex-valued analog beamformer
def trans_Vrf(temp):
    v_real = tf.cos(temp)
    v_imag = tf.sin(temp)
    vrf = tf.cast(tf.complex(v_real, v_imag), tf.complex64)
    return vrf

batchsize = int(input("please enter the batchsize you want to run the model"))
print("the batch size of the model is -->" + str(batchsize))
# For the simplification of implementation based on Keras, we use a lambda layer to compute the rate
# Thus, the output of the model is actually the loss.
def Rate_func(temp):
    h, v, SNR_input = temp
    #here we split the hte h and the V_rf we get for getting individual users
    print("\n the shape of the h is----- ")
    print(h.shape)
    #batchsize = 50
    h_1, h_2 = tf.split(h, num_or_size_splits = 2, axis = 1, name = 'split')
    v_1, v_2 = tf.split(v, num_or_size_splits = 2, axis = 1, name = 'split')
    print("\n the shape of the h_1")
    print(h_1.shape)
    print("\n the shape of the v_1")
    print(v_1.shape)
    H =tf.concat((h_1, h_2), 0)
    print("\n the shape of the H cap in rate func")
    print(H.shape)
    V =tf.transpose(tf.concat((v_1, v_2), 0))
    print("\n the shape of the v in rate func")
    print(V.shape)
    mat = tf.linalg.matmul(H,V)
    row = tf.zeros([1,(2*batchsize)], dtype = tf.complex64)
    for i in range(0,(2*batchsize),2):
        if(i == 0):
            z_2 = tf.zeros([2,(2*batchsize) - 2], dtype = tf.complex64)
            ele = tf.gather(tf.gather(mat,indices = range(0,2)), indices = range(0,2), axis = 1)
            row_gen = tf.concat([ele,z_2], axis = 1)
            #print("the shape of row_gen--",row_gen.shape)
        elif(i == ((2*batchsize)-2)):
            z_1 = tf.zeros([2,(2*batchsize)-2], dtype = tf.complex64)
            ele = tf.gather(tf.gather(mat,indices = range(i,i+2)), indices = range(i,i+2), axis = 1)
            row_gen = tf.concat([z_1,ele], axis = 1)
        else:
            z_1 = tf.zeros([2,i], dtype = tf.complex64)
            z_2 = tf.zeros([2,(2*batchsize - (i+2))], dtype = tf.complex64)
            ele = tf.gather(tf.gather(mat,indices = range(i,i+2)), indices = range(i,i+2), axis = 1)
            row_gen = tf.concat([z_1,ele,z_2], axis = 1) 
        row = tf.concat([row,row_gen],0)
        
    indicesn = range(1,(2*batchsize+1))
    mat = tf.gather(row, indicesn, axis = 0)
    print("the shape of the mat is --",mat.shape)
    v_d = tf.linalg.inv(mat)
    print("\n the shape of the V_d in rate func")
    print(v_d.shape)
    #print(v_d)
    X = tf.linalg.matmul(V,v_d)
    print("\n the shape of the X in rate func")
    print(X.shape) 
    arr = tf.zeros([1,1], dtype = tf.float32)
    j = 0
    for i in range(0,(2*batchsize),2):
        indices_0 = [i]
        indices_1 = [i+1]
        indices_2 = [j]
        op_0 = tf.gather(X, indices_0, axis=1)
        op_1 = tf.gather(X, indices_1, axis=1)
        op_0 = 1/((2)*tf.pow(tf.norm(tf.abs(op_0)),2))
        op_1 = 1/((2)*tf.pow(tf.norm(tf.abs(op_1)),2))
        snr = tf.gather(SNR_input,indices_2,axis = 0)
        op_0 = tf.math.log(1+(snr*op_0))/tf.math.log(2.0)
        op_1 = tf.math.log(1+(snr*op_1))/tf.math.log(2.0)
        op = op_0 + op_1
        arr = tf.concat([arr,op], axis = 0)
        j += 1
    
    print("\n the shape of the array is")
    print(arr.shape)
    indicesp = range(1,(batchsize)+1)
    P_mat = tf.gather(arr, indicesp, axis=0)
    print("#########################")
    print("the shape of the P_mat is ")
    print(P_mat.shape)
    #rate_new = tf.cast(tf.reduce_sum(P_mat, axis = 1),tf.float32)
    rate_new = P_mat
    #print("the rate_new")
    #print(rate_new)
    #print(rate_new)
    return -rate_new

def power_func(temp):
	v, SNR_input = temp
	print("------------------------------------",v)
	print("-------------------------------------",SNR_input)
	#vSNR_imput = tf.keras.backend.batch_dot(, SNR_input)
	power = (v*tf.cast(SNR_input,tf.complex64)*tf.cast(tf.math.sqrt(2.), tf.complex64))/(2*Nt)
	power=tf.pow((tf.abs(power)),2)
	print("--------------------------------",power)
	#return power
	return power
	
# load the saved .mat files generated by Matlab.
#path = 'C:/Users/Vaishnavi/Desktop/Beamforming_2users_W02/train_set/example/Train'
#path = 'C:/Users/Vaishnavi/Desktop/Bf_revised/train_set/example/Train'
def mat_load(path):
    print('loading data...')
    # load the perfect csi
    h_1 = sio.loadmat(path + '/H_1.mat')['val']
    # load the estimated csi
    h_1_est = sio.loadmat(path + '/H_1_est.mat')['val']
    # load the perfect csi for the second user
    h_2 = sio.loadmat(path + '/H_2.mat')['val']
    #load the estimated csi for the second user
    h_2_est = sio.loadmat(path + '/H_2_est.mat')['val']
    print('loading complete')
    print('The shape of CSI is: ', h_1_est.shape)
    return h_1, h_1_est, h_2, h_2_est
